A multithreaded web crawler designed to extract and analyze web content efficiently. Written in Python. 

Features
	•	Multithreaded crawling for faster performance.
	•	Adheres to robots.txt policies for ethical crawling.
	•	Detects and avoids crawler traps.
	•	URL deduplication using SimHash.
	•	Extracts most common words and identifies the longest page by word count.

Acknowledgements
  • Created as part of UC Irvine's Information Retrieval Course
